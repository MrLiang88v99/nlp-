# 分词

## 中文分词

中文分词分为三个类别模型的实现，首先采用传统机器学习的LSTM模型对数据训练，然后采用深度学习里的CRF进行进行再次训练，了解了预处理模型base模型的应用。

### LSTM

#### 1.词表创建

首先我们需要将字统计出来，读取成字典，处理成词表

```python
# 读取字典   
vocab = open('C:\\Users\\26233\\Desktop\\nlp\\dataset\\icwb2-data\\gold\\pku_training_words.txt').read().rstrip('\n').split('\n')  # 去除右边的换行，并按换行分割

vocab = list(''.join(vocab))  # 将所有词连接成一个大的字符串，然后转换为list  也就是说列表中每个元素为对应的字符
stat = {}
for v in vocab:
    stat[v] = stat.get(v, 0) + 1
stat = sorted(stat.items(), key=lambda x: x[1], reverse=True)
vocab = [s[0] for s in stat]  # 取出每一个字
print(len(vocab))  # 看一下总的字个数
# 映射
char2id = {c: i + 1 for i, c in enumerate(vocab)}
id2char = {i + 1: c for i, c in enumerate(vocab)}
# 这个tags就是我们最后的标签。
tags = {'s': 0, 'b': 1, 'm': 2, 'e': 3, 'x': 4}  # 进行的一些标注  x为填充的
```

#### 2.定义模型所需要的一些参数

这里我们对模型的参数进行设定，设置了迭代50次，同时把字镶嵌长度给设置整理出来。

```python
embedding_size = 128  # 字嵌入的长度
maxlen = 32  # 长于32则截断，短于32则填充0
hidden_size = 64
batch_size = 64
epochs = 50
```

#### 3.数据读取处理

我们需要将数据按标点符号分割的同时再分割，将数据分割传唤成one-hot编码，以便后续数据的训练与使用。在这里我们定义一个函数对该方法进行实现。

```python
def load_data(path):
    data = open(path).read().rstrip('\n')
    # 按标点符号和换行符分隔
    data = re.split('[，。！？、\n]', data)  # 按 ,。！？、 \n 对数据进行分割  就是将一些句子切分成很小的一句话
    print('共有数据 %d 条' % len(data))  # 385152条数据
    print('平均长度：', np.mean([len(d.replace(' ', '')) for d in data]))  # 大概是9.74

    # 准备数据
    X_data = []
    y_data = []

    for sentence in data:
        sentence = sentence.split(' ')  # 按空格切分这些已经被切分的很小的句子
        X = []
        y = []

        try:
            for s in sentence:
                s = s.strip()
                # 跳过空字符
                if len(s) == 0:
                    continue
                # s
                elif len(s) == 1:
                    # 一个字能够独立构成词的话  标记为s
                    X.append(char2id[s])
                    y.append(tags['s'])
                elif len(s) > 1:
                    # 如果多个字构成一个词， 第一个字标记为b  最后一个字标记为e  中间所有的字标记为m
                    # b
                    X.append(char2id[s[0]])
                    y.append(tags['b'])
                    # m
                    for i in range(1, len(s) - 1):
                        X.append(char2id[s[i]])
                        y.append(tags['m'])
                    # e
                    X.append(char2id[s[-1]])
                    y.append(tags['e'])

            # 统一长度    一个小句子的长度不能超过32,否则将其切断。只保留32个
            if len(X) > maxlen:
                X = X[:maxlen]
                y = y[:maxlen]
            else:
                for i in range(maxlen - len(X)):  # 如果长度不够的，我们进行填充，记得标记为x
                    X.append(0)
                    y.append(tags['x'])
        except:
            continue
        else:
            if len(X) > 0:
                X_data.append(X)  # [ [ [   每个词语中字转换为对应的id构成的列表     ], [], [], [] ...], [], [], [], []...]
                y_data.append(y)  # [ [ [每个词语中对应字的标注,有四种标记，构成的列表], [], [], []...], [], [], []...]

    X_data = np.array(X_data)
    y_data = np_utils.to_categorical(y_data, 5)  # 将y搞成one_hot编码

    return X_data, y_data
```

#### 4.对数据进行训练集，测试集的划分。

```python
X_train, y_train = load_data('C:\\Users\\26233\\Desktop\\nlp\\dataset\\icwb2-data\\training\\pku_training.txt')  # 读取的这个文档是一个已经分好词的文本
X_test, y_test = load_data('C:\\Users\\26233\\Desktop\\nlp\\dataset\\icwb2-data\\gold\\pku_test_gold.txt')
print('X_train size:', X_train.shape)  # (385152, 32)
print('y_train size:', y_train.shape)  # (385152, 32, 6)
print('X_test size:', X_test.shape)  # (17917, 32)
print('y_test size:', y_test.shape)  # (17917, 32, 5)
```

#### 5.定义模型

定义输入层，输入的是一个固定长度的序列，使用int32类型存储

```python
X = Input(shape=(maxlen,), dtype='int32')
```

使用Embedding层将每个字符串转换为维度为embedding_size的向量，  maxlen为序列的最大长度，在序列不够长的情况下使用0进行填充 ，mask_zero=True的作用为忽略填充0的存在。

```python
embedding = Embedding(input_dim=len(vocab) + 1, output_dim=embedding_size, input_length=maxlen, mask_zero=True)(X) 
```

将正向的LSTM和反向的LSTM的输出拼接在一起

```python
blstm = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(embedding)
```

在blstm层之后加上Dropout层以防止过拟合

```python
blstm = Dropout(0.6)(blstm) 
```

再次将正向LSTM和反向LSTM的输出拼接在一起 

```python
blstm = Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode='concat')(blstm) 
```

仍然加上Dropout层 

```python
blstm = Dropout(0.6)(blstm) 
```

#使用TimeDistributed层将blstm层的输出中的每一个单元进行softmax映射 ,这样每一个单元的输出就是分类概率 output = TimeDistributed(Dense(5, activation='softmax'))(blstm) ,此时的output形状为(batch_size, 32, 5) ,使用Model类将输入和输出连接起来形成一个模型 model = Model(X, output) ,编译模型，定义损失函数和优化器

```python
 model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) 
```

#训练模型 

```python
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs) 
```

#保存模型 

```python
model.save('pku_bilstm.h5')
```

6.分词函数

首先定义一个寻找最大路径的函数

```python
def viterbi(nodes):
    # 定义转移概率矩阵，初始状态概率为0.5
    trans = {'be': 0.5, 'bm': 0.5, 'eb': 0.5, 'es': 0.5, 'me': 0.5, 'mm': 0.5, 'sb': 0.5, 'ss': 0.5}
    # 定义初始路径，分别对应B和S状态
    paths = {'b': nodes[0]['b'], 's': nodes[0]['s']}
    # 逐个遍历字符
    for l in range(1, len(nodes)):
        paths_ = paths.copy()  # 前一时刻的路径
        paths = {}  # 当前时刻的路径
        for i in nodes[l].keys():  # 遍历当前时刻的状态
            nows = {}  # 当前时刻每个状态的最大概率
            for j in paths_.keys():  # 遍历前一时刻的状态
                if j[-1] + i in trans.keys():  # 判断状态转移是否合法
                    nows[j + i] = paths_[j] + nodes[l][i] + trans[j[-1] + i]  # 计算当前状态最大概率
            nows = sorted(nows.items(), key=lambda x: x[1], reverse=True)  # 对当前时刻每个状态的最大概率从大到小排序
            paths[nows[0][0]] = nows[0][1]  # 取当前最大概率的状态作为当前时刻的路径
    paths = sorted(paths.items(), key=lambda x: x[1], reverse=True)  # 对最终路径概率从大到小排序
    return paths[0][0]  # 返回最大概率的路径
```

然后再定义一个切分的函数

```python
def cut_words(data):
    # 使用re.split()方法进行切分，切分符包括逗号、句号、感叹号、问号、顿号以及换行符
    data = re.split('[，。！？、\n]', data)
    # 定义两个空列表，一个用于存储所有切分后的小句子，一个用于存储每个小句子的词语列表（用于后续的输入预处理）
    sens = []
    Xs = []
    # 遍历每个小句子
    for sentence in data:
        # 定义两个空列表，一个用于存储当前小句子中的所有词语，一个用于存储当前小句子中的所有词语对应的id
        sen = []
        X = []
        # 将当前小句子转换为列表，里面的每个元素为一个字
        sentence = list(sentence)
        # 遍历当前小句子中的每个字
        for s in sentence:
            # 去除字符串两端的空白字符
            s = s.strip()
            # 如果当前的字符不为空且这个字符在字典char2id中存在，则将其加入到词语列表和词语id列表中
            if not s == '' and s in char2id:
                sen.append(s)
                X.append(char2id[s])
        # 如果当前小句子的长度超过了设定的最大长度，则将其截断
        if len(X) > maxlen:
            sen = sen[:maxlen] 
            X = X[:maxlen]
        # 如果当前小句子的长度不足最大长度，则将其补齐
        else:
            for i in range(maxlen - len(X)):
                X.append(0)
        # 如果当前小句子中有词语，则将其加入到Xs和sens列表中
        if len(sen) > 0:
            Xs.append(X)
            sens.append(sen)
    # 将Xs列表转化为numpy数组，并将其输入到神经网络模型中进行预测
    Xs = np.array(Xs)
    ys = model.predict(Xs)
    # 定义一个空字符串，用于存储最终的分词结果
    results = ''
    # 遍历每个小句子的预测结果
    for i in range(ys.shape[0]):
        # 将每个字的预测结果与SBME对应构成字典格式[{s:, b:, m:, e:}, {}, {}...]
        nodes = [dict(zip(['s', 'b', 'm', 'e'], d[:4])) for d in ys[i]]
        # 使用维特比算法对当前小句子进行最优路径的计算
        ts = viterbi(nodes)
        # 遍历当前小句子中的每个字
        for x in range(len(sens[i])):
            # 如果当前字对应的标记是S或E，则在结果字符串中添加当前字和一个斜杠
            if ts[x] in ['s', 'e']:
                results += sens[i][x] + '/'
            # 否则，在结果字符串中仅添加当前字
            else:
                results += sens[i][x]
    # 返回分词结果字符串（去掉最后一个斜杠）
    return results[:-1]
```

#### 6结果展示

![image-20230613190039690](C:\Users\26233\AppData\Roaming\Typora\typora-user-images\image-20230613190039690.png)

可以看出其在测试集上达到有百分之91的准确率，并且在分词时也基本可以快速响应。

### CRF

#### 1.处理数据

我们需要将数据转换为BMES存储才能够接下来进行相应的处理操作。

```python
def character_tagging(input_file,output_file):
    # 打开输入文件和输出文件
    input_data = open(input_file,'r',encoding='UTF-8')
    output_data = open(output_file,'w',encoding='UTF-8')
    # 遍历输入文件的每一行
    for line in input_data.readlines():
        # 将该行文本按空格分隔为列表
        word_list = line.strip().split()
        # 遍历列表中的每一个词语
        for word in word_list:
            # 判断词语长度是否为1，如果是，直接将该词语与标记" S"写入输出文件中
            if len(word)==1:
                output_data.write(word+'\tS\n')
            else:
                # 如果词语长度大于1，将该词语的第一个字符与标记" B"写入输出文件中
                output_data.write(word[0]+'\tB\n')
                # 遍历该词语剩余的字符，将每个字符与标记" M"写入输出文件中
                for w in word[1:len(word)-1]:
                    output_data.write(w+'\tM\n')
                # 将该词语的最后一个字符与标记" E"写入输出文件中
                output_data.write(word[len(word)-1]+'\tE\n')
        # 在每一行的末尾写入一个换行符
        output_data.write('\n')
    # 关闭输入文件和输出文件
    input_data.close()
    output_data.close()

定义一个字符切分函数，输入文件和输出文件
def character_split(input_file, output_file):
    # 打开输入文件和输出文件
    input_data = open(input_file, 'r',encoding='UTF-8-sig')
    output_data =open(output_file, 'w',encoding='UTF-8-sig')
    # 遍历输入文件的每一行
    for line in input_data.readlines():
        # 遍历该行的每一个字符
        for word in line.strip():
            # 去除该字符前后的空白字符
            word = word.strip()
            # 如果该字符不为空白字符，将该字符与标记" B"写入输出文件中
            if word:
                output_data.write(word + "\tB\n")
        # 在每一行的末尾写入一个换行符
        output_data.write("\n")
    # 关闭输入文件和输出文件
    input_data.close()
    output_data.close()   
```

然后通过crf++这个工具进行crf模型的训练。

#### 2.进行训练

采用crf++工具。

进入cmd，输入

```
crf_test -m crf_model test_out.utf8 > out_crftag.utf8
```

#### 3.还原原来文本

定义函数character_2_word，以实现从基于字符的标注数据到基于词的标注数据的转换。

```python
def character_2_word(input_file,output_file):
    # 打开输入文件，以获得读取权限，并使用UTF-8-sig编码
    input_data = open(input_file, 'r',encoding= 'UTF-8-sig')
    # 打开输出文件，以获得写入权限，并使用UTF-8-sig编码
    output_data = open(output_file, 'w', encoding='UTF-8-sig')
    # 对于输入文件的每一行数据
    for line in input_data.readlines():
        # 若该行为空，则将空行写入输出文件
        if line == "\n":
            output_data.write("\n")
        else:
            # 将字符标注对按tab为分隔符进行分割，以便提取字符和标签
            char_tag_pair = line.strip().split('\t')
            # 获得当前字符
            char = char_tag_pair[0]
            # 获得当前标签
            tag = char_tag_pair[2]
            # 根据当前标签的不同，决定字符的写入方式
            if tag == 'B': # 若当前标签为B，则表示该字符是一个词的开始
                output_data.write(' ' + char) # 将空格和该字符写入输出文件
            elif tag == 'M': # 若当前标签为M，则表示该字符是一个词的中间部分
                output_data.write(char) # 直接将该字符写入输出文件
            elif tag == 'E': # 若当前标签为E，则表示该字符是一个词的结尾部分
                output_data.write(char + ' ') # 将该字符和空格写入输出文件
            else: # tag == 'S'，若当前标签为S，则表示该字符是一个单独的词
                output_data.write(' ' + char + ' ') # 将空格、该字符和空格写入输出文件
    # 关闭输入文件和输出文件，以释放资源
    input_data.close()
    output_data.close()
```

4.模型相关参数计算。

定义一个求精度的函数accuracy()，读取分词结果文件和标准答案文件，统计分词结果和标准答案的总词数count_test和count_gold，遍历分词结果和标准答案，比较每个词是否相同，若相同则count加1，若分词结果和标准答案中相邻两个词的长度不同，则调整指针i和j的位置保持一致，计算准确率Precision，召回率Recall和F值，输出结果并返回Precision，Recall和F值。

```python
def accuracy():
    #读取测试集和标准答案
    fp=open("C:\Users\26233\Desktop\nlp\dataset\icwb2-data\gold\pku_test_gold.utf8","r",encoding="UTF-8-sig")
    count=0 #计数器
    count_gold=0 #金标准总词数计数器
    count_test=0 #测试集总词数计数器
    #读取分词结果
    with open("F:\nlp\dataset\fenci\fenci-test-huanyuan.txt","r",encoding="UTF-8-sig") as lines_total:
        for line1 in lines_total:
            i=0
            j=0
            if len(line1) == 0:          #为空时 结束遍历
                fp.readline()
                continue
            line1=line1.split()
            count_test+=len(line1)
            line2=fp.readline()
            if len(line2) == 0:          #为空时 结束遍历
                continue
            line2=line2.split()
            count_gold+=len(line2)
            while i< len(line1) and j<len(line2): #遍历两个列表，计算正确率
                if line1[i]==line2[j]:
                    count+=1                 #计数
                    i=i+1
                    j=j+1
                else:
                    len_i = len(line1[i])
                    len_j = len(line2[j])
                    while len_i != len_j:    #如果两个列表当前词不匹配，则继续遍历，找到匹配的词
                        if len_i>len_j:
                            j=j+1
                            len_j = len_j+len(line2[j])
                        else :
                            i=i+1
                            len_i=len_i+len(line1[i])
                    i+=1
                    j+=1
    fp.close()  #关闭文件
    lines_total.close() #关闭文件
    #计算精确度
    Precision=count/count_test
    Recall=count/count_gold
    total=Precision+Recall
    F=(2PrecisionRecall)/total
    #输出精确度结果
    print('CRF分词结束，计算精确度')
    print('评测结果：\n')
    print('正确率：%-12.20f\n召回率：%-12.20f\nF值：%-12.20f\n'%(Precision,Recall,F))
    return(Precision,Recall,F)
```

结果展示

![image-20230613191804575](C:\Users\26233\AppData\Roaming\Typora\typora-user-images\image-20230613191804575.png)

可见其准确率在百分之94左右。
